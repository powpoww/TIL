{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03.언어 모델.ipynb","provenance":[],"collapsed_sections":["bXc8DcqS8KWl","E7JIYqvB8hy5","q-f7e43JfUTz","QgZ1uRdXfXBp","GKotOSqnfa7z","_IFO1YQdfgPD","XZ4qHnE48hvk","hpabgpO9kN9e","F7P0iZNmlTfC","dJtRuiZKnqqJ","J4-Up_ky8hqR","mCXDYz3oowfO","uizTmLdupLIT","df0GpEx5pOFK","HlfgDu9opRJd","Q12r2xdmpUrm","5E1sGiURwGXp","fwHKy2gQwOD6","tHjYR1yw22FU","dB3P8h_a3vsr"],"authorship_tag":"ABX9TyMu9siDBhCQJCm1p4GLTWDM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 0) 서론"],"metadata":{"id":"bXc8DcqS8KWl"}},{"cell_type":"markdown","source":["- 언어 모델: 문장에 확률을 할당하는 모델\n","- 기계가 이 문장이 적절한지(말이 되는지 여부)를 판단하는 모델"],"metadata":{"id":"GgAlVocy8Nxn"}},{"cell_type":"markdown","source":["# 1)언어 모델(Language Model)이란?"],"metadata":{"id":"E7JIYqvB8hy5"}},{"cell_type":"markdown","source":["- 언어 모델: 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델\n","- 방법\n","  1. 통계를 이용한 모델 \n","  2. 인공 신경망을 이용한 방법"],"metadata":{"id":"Ogf-j90686Sg"}},{"cell_type":"markdown","source":["## 1.언어 모델(Language Model)"],"metadata":{"id":"q-f7e43JfUTz"}},{"cell_type":"markdown","source":["- 언어 모델: \n","  - 단어 시퀀스에 확률을 할당(assign)\n","  - 가장 자연스러운 단어 시퀀스를 찾는 모델\n","  - 빈칸 추론과 유사\n","  - 다음 단어를 예측하는 일"],"metadata":{"id":"9TuK5BxAfr02"}},{"cell_type":"markdown","source":["## 2.단어 시퀀스의 확률 할당"],"metadata":{"id":"QgZ1uRdXfXBp"}},{"cell_type":"markdown","source":["- why  단어 시퀀스에 확률 할당이 필요 할까?\n","  - 보다 적절한 문장을 판단하려고\n","- a.기계 번역:\n","  - P(버스를 탔다)>P(버스를 태운다)\n","- b.오타 교정\n","  - 선생님이 교실로 부리나케\n","  - P(달려갔다)>P(잘려갔다)\n","- c.음성 인식\n","  - P(메론을 먹는다)>P(메롱을 먹는다)"],"metadata":{"id":"2060HUkbgSrZ"}},{"cell_type":"markdown","source":["## 3.주어진 이전 단어들로부터 다음 단어 예측하기"],"metadata":{"id":"GKotOSqnfa7z"}},{"cell_type":"markdown","source":["- 언어모델에 확률을 할당할때 이전 단어를 기반으로 다음 단어 예측이 보편적임\n","  - 조건부 확률을 이용함\n","1. 단어 시퀀스의 확률\n","  - P(W)=P(w1,w2,...,wn)\n","    - 하나의 단어: w\n","    - 단어 시퀀스: W\n","2. 다음 단어 등장 확률\n","  - P(wn)=P(wn|w1,w2,...w(n-1))\n","    - n-1개의 단어가 나열된 상태어서, n번째 단어의 확률\n","  - P(W)=P(w1,w2,...,wn)=P(w1)부터P(wn)의 곱"],"metadata":{"id":"rbP7anSEhcHt"}},{"cell_type":"markdown","source":["## 4.언어 모델의 간단한 직관\n"],"metadata":{"id":"_IFO1YQdfgPD"}},{"cell_type":"markdown","source":["- 앞의 내용이 자세할 수록 더 잘 예측한다.\n","  - 비행기를 [?]\n","  - 지각을 하는 바람에 비행기를 [?]\n","  - 놓쳤다."],"metadata":{"id":"mxB9WkUYji1H"}},{"cell_type":"markdown","source":["# 2)통계적 언어 모델(Statistical Language Model, SLM)"],"metadata":{"id":"XZ4qHnE48hvk"}},{"cell_type":"markdown","source":["## 1.조건부 확률"],"metadata":{"id":"0uTL7Evvj02c"}},{"cell_type":"markdown","source":["## 2.문장에 대한 확률"],"metadata":{"id":"hpabgpO9kN9e"}},{"cell_type":"markdown","source":["- 각 단어: 문맥이란 관계 => 이전 단어에 영향 받음\n","  - 따라서 조건부 확률의 사용이 가능\n","- P(나는 배가 고프다.)=P(나는)*P(배가|나는)*P(고프다.|나는 배가)"],"metadata":{"id":"lW1yfGAYkXBx"}},{"cell_type":"markdown","source":["## 3.카운트 기반의 접근"],"metadata":{"id":"F7P0iZNmlTfC"}},{"cell_type":"markdown","source":["- 다음 단어의 확률은 어떻게 구할까?\n","- P(배가|나는)=P(나는 배가)/P(나는)\n","- '나는'이 100번 등장하고 그 다음 '배가'가 등장한 경우가 30번이면, P(배가|나는)=0.3이다."],"metadata":{"id":"e9BxZGFJmnOI"}},{"cell_type":"markdown","source":["## 4.카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)"],"metadata":{"id":"dJtRuiZKnqqJ"}},{"cell_type":"markdown","source":["- 희소 문제:\n","  - 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제\n","- SLM은 방대한 양의 훈련이 필요하다.\n","- '나는 배가' 뒤에 '고프다'라는 말이 나온 경우가 없다면 확률이 0이 됨\n","\n","- 일반화(generalization) 기법\n","  - 희소문제를 완화"],"metadata":{"id":"cW8WYmZ-nz7I"}},{"cell_type":"markdown","source":["# 3)N-gram 언어 모델(N-gram Language Model)"],"metadata":{"id":"J4-Up_ky8hqR"}},{"cell_type":"markdown","source":["- n-gram 언어 모델: \n","  - 카운트에 기반한 통계적 접근을 사용\n","  - SLM의 일종\n","  - But 일부 단어만 고려하는 접근 방법\n","  "],"metadata":{"id":"qupW4vx5pIq3"}},{"cell_type":"markdown","source":["## 1.코퍼스에서 카운트하지 못하는 경우의 감소."],"metadata":{"id":"mCXDYz3oowfO"}},{"cell_type":"markdown","source":["- 문장이 길어질수록 SLM은 계산하려는 문장이나 단어가 없을 확률이 올라감\n","- P(is|An adorable little boy)=P(is|boy)로 가정\n","- 또는 P(is|An adorable little boy)=P(is| little boy)로 가정\n","  - 희소문제가 낮아짐"],"metadata":{"id":"3MsUODfKrTlG"}},{"cell_type":"markdown","source":["## 2.N-gram"],"metadata":{"id":"uizTmLdupLIT"}},{"cell_type":"markdown","source":["-  n개의 연속적인 단어 나열\n","- n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주\n","- 예\n","  - unigrams : an, adorable, little, boy, is, spreading, smiles\n","  - bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n","- n-gram을 통한 언어 모델: 오직 n-1개의 단어에만 의존\n"],"metadata":{"id":"0zjA4ZBpt8eU"}},{"cell_type":"markdown","source":["## 3.N-gram Language Model의 한계"],"metadata":{"id":"df0GpEx5pOFK"}},{"cell_type":"markdown","source":["- 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생김\n","1. 희소 문제(Sparsity Problem)\n","  - SLM보다는 낮지만 여전히 존재\n","2. n을 선택하는 것은 trade-off 문제.\n","  - n은 최대 5를 넘게 잡아서는 안 된다고 권장"],"metadata":{"id":"WBPnX2gluukA"}},{"cell_type":"markdown","source":["## 4.적용 분야(Domain)에 맞는 코퍼스의 수집"],"metadata":{"id":"HlfgDu9opRJd"}},{"cell_type":"markdown","source":["- 어떤 분야인지 어껀 상황인지에 따라 특정 단어의 확률이 변함"],"metadata":{"id":"hYG16nRBvm3H"}},{"cell_type":"markdown","source":["## 5.인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)"],"metadata":{"id":"Q12r2xdmpUrm"}},{"cell_type":"markdown","source":["- -  N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용"],"metadata":{"id":"hWotwTc8wD0O"}},{"cell_type":"markdown","source":["# 4)한국어에서의 언어 모델(Language Model for Korean Sentences)"],"metadata":{"id":"5E1sGiURwGXp"}},{"cell_type":"markdown","source":["1. 한국어는 어순이 중요하지 않다.\n","  - 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵다.\n","2. 한국어는 교착어이다.\n","  - 언어모델의 작동을 어렵게함\n","  - 한국어에서는 토큰화를 통해 접사나 조사 등을 분리하는 것은 중요한 작업\n","3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.\n","  - 토큰이 제대로 분리 되지 않는채 훈련 데이터로 사용된다면 언어 모델은 제대로 동작하지 않는다."],"metadata":{"id":"EjxPjWpz1sUO"}},{"cell_type":"markdown","source":["# 5)펄플렉서티(Perplexity, PPL)"],"metadata":{"id":"fwHKy2gQwOD6"}},{"cell_type":"markdown","source":["- 모델의 성능은 어떻게 비교할 수 있을까?"],"metadata":{"id":"EZ7bJ73W2sKV"}},{"cell_type":"markdown","source":["## 1.언어 모델의 평가 방법(Evaluation metric) : PPL"],"metadata":{"id":"tHjYR1yw22FU"}},{"cell_type":"markdown","source":["- PPL: \n","  - 헷갈리는 정도\n","  - '낮을수록' 언어 모델의 성능이 좋다는 것\n","\n","- PPL(W)=P(w1,...,wN)\\^-(1/N)\n"],"metadata":{"id":"ATZMVt6L25zf"}},{"cell_type":"markdown","source":["## 2.분기 계수(Branching factor)"],"metadata":{"id":"dB3P8h_a3vsr"}},{"cell_type":"markdown","source":["- PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수\n","- PPL의 값이 낮다는 것:\n","  - 테스트 데이터 상에서 높은 정확도를 보인다는 것 0\n","  - 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다"],"metadata":{"id":"jG810bcC31XQ"}},{"cell_type":"markdown","source":["## 3.기존 언어 모델 Vs. 인공 신경망을 이용한 언어 모델."],"metadata":{"id":"GE5gv3qu4cOq"}},{"cell_type":"markdown","source":["- https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words\n","- 페이스북 연구팀에 의하면 인공신경망이 우수하다."],"metadata":{"id":"dUJKiL604oXY"}},{"cell_type":"markdown","source":["# 6)조건부 확률(Conditional Probability)"],"metadata":{"id":"pwBS6b2twPwD"}}]}